# -*- coding: utf-8 -*-
"""1 - Preguntas_bbdd_dbcoders_v2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1btS3CJBQugKamcuiN6ht76FS-rxkK7RJ

#**Resumen de cuestionario en topicos para twitter**

## Importamos librerías
"""

import nltk
nltk.download('punkt')
from nltk.tokenize import word_tokenize
from nltk import tokenize
import numpy as np

nltk.download('stopwords')
from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))

! pip install -U sentence-transformers

from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans
import pandas as pd
import re
from scipy.spatial.distance import cdist
import matplotlib.pyplot as plt

"""# Conecto con drive"""

from google.colab import drive
drive.mount('/content/drive')

path='drive/My Drive/Proyecto_BD_coders/Dataset/'
path_rdo='drive/My Drive/keepcoding/Tweets/resultados/'

"""# Importo preguntas cuestionario"""

path+'Columnas_Descripcion.csv'

questions=pd.read_csv(path+'Columnas_Descripcion.csv', header=None, sep=';',usecols=[0, 1],names=['key','question'])
questions.head()

df_clean=pd.read_csv(path+'df_clean.csv')

questions_clean=df_clean.columns
questions_clean=questions_clean[2:-1]
questions_clean

# me quedo con las claves y las preguntas que han pasado el filtro de la limpieza de datos, las que se corresponden con df_clean
df=questions[questions['key'].isin(questions_clean)]
df.head()

"""## Limpiamos el cuestionario

Elimino los textos de preguntas que no aportan nada
"""

strings = ['FINAL TRIMMED NONRESPONSE ADJUSTED STUDENT REPLICATE BRR-FAY WEIGHTS','Country Identifier','Country code 3-character',
           'Intl. School ID','Intl. Student ID',]

questions_erased=[]
rgx = '\\b(?:' + '|'.join(strings) + ')\\b'
questions_erased.append(df[df['question'].str.contains(rgx, regex=True, na=False)]['key'])
df_questions=df[~df['question'].str.contains(rgx, regex=True, na=False)]

df_questions=df_questions.drop_duplicates()

# Elimino de las preguntas la referencia al centro en el que estudió:  <ISCED level 6> por ejemplo
df_questions['question']=df_questions['question'].str.replace('<',' ')
df_questions['question']=df_questions['question'].str.replace('>',' ')
df_questions['question']=df_questions['question'].str.replace('ISCED',' ')
df_questions.head(5)

"""# Clusterizo las preguntas"""

"""
This is a simple application for sentence embeddings: clustering

Sentences are mapped to sentence embeddings and then k-mean clustering is applied.
"""

embedder = SentenceTransformer('paraphrase-MiniLM-L6-v2')

corpus=list(df_questions['question'])
corpus_embeddings = embedder.encode(corpus)

# Perform kmean clustering

distortions = []
inertias = []
mapping1 = {}
mapping2 = {}
K = range(1, 10)
  
for k in K:
    # Building and fitting the model
    clustering_model = KMeans(n_clusters=k)
    clustering_model.fit(corpus_embeddings)
  
    distortions.append(sum(np.min(cdist(corpus_embeddings, clustering_model.cluster_centers_,
                                        'euclidean'), axis=1)) / corpus_embeddings.shape[0])
    inertias.append(clustering_model.inertia_)
  
    mapping1[k] = sum(np.min(cdist(corpus_embeddings, clustering_model.cluster_centers_,
                                   'euclidean'), axis=1)) / corpus_embeddings.shape[0]
    mapping2[k] = clustering_model.inertia_

plt.plot(K, distortions, 'bx-')
plt.xlabel('Values of K')
plt.ylabel('Distortion')
plt.title('The Elbow Method using Distortion')
plt.show()

plt.plot(K, inertias, 'bx-')
plt.xlabel('Values of K')
plt.ylabel('Inertia')
plt.title('The Elbow Method using Inertia')
plt.show()

# selecciono el numero de clusters
num_clusters = 5

embedder = SentenceTransformer('paraphrase-MiniLM-L6-v2')

corpus=list(df_questions['question'])
corpus_embeddings = embedder.encode(corpus)

# Perform kmean clustering

clustering_model = KMeans(n_clusters=num_clusters)
clustering_model.fit(corpus_embeddings)
cluster_assignment = clustering_model.labels_

clustered_sentences = [[] for i in range(num_clusters)]
for sentence_id, cluster_id in enumerate(cluster_assignment):
    clustered_sentences[cluster_id].append(corpus[sentence_id])

for i, cluster in enumerate(clustered_sentences):
    print("Cluster ", i+1)
    print(cluster)
    print("")

"""# Resumo los clusters por palabras/tema para su búsqueda en twitter"""

len(stop_words)

# añado stop words propias, muchas provienen de la primera parte de la pregunta que se repite para varias preguntas y  no nos permite extraer las palabras más representativas del cluster
stop_words_custom={'school','use','using','lessons','classroom','learnt', 'heard', 'read','plausible','value','subscale','following','agree','well','...','e.g','wle','describe','get','often','teacher','test','last','12','months','10','many','education', '5a', 'highest','typical','Find','future', 'study', 'types','work',
                   'feel','Plausible', 'Value','Global', 'Competency','global', 'competency','reading','term', 'student', 'thinking', 'students'}
stop_words.update(stop_words_custom)

def summarize_questions(text):
  sentences = []
  for text_part in text:
    sentences.extend(tokenize.sent_tokenize(text_part))
  word_weights={}
  for sent in sentences:
    for word in word_tokenize(sent):
      word = word.lower()
      if len(word) > 1 and word not in stop_words:
        if word in word_weights.keys():
          word_weights[word] += 1
        else:
          word_weights[word] = 1
  sentence_weights={}
  for sent in sentences:
    sentence_weights[sent] = 0
    tokens = word_tokenize(sent)
    for word in tokens:  
      word = word.lower()
      if word in word_weights.keys():
        sentence_weights[sent] += word_weights[word]
      highest_weights = sorted(sentence_weights.values())[-1:]
  summary=""
  for sentence,strength in sentence_weights.items():
    if strength in highest_weights:
      print(sentence)
      summary += sentence + " "
  summary = summary.replace('_', ' ').strip()
  print(summary)
  return summary,word_weights

summary_clusters=[]
word_weights_cluster=pd.DataFrame()
clustered_sentences_df=pd.DataFrame()
for i, cluster in enumerate(clustered_sentences):
  summary, word_weights = summarize_questions(cluster)
  summary_clusters.append(summary)
  word_weights=pd.DataFrame.from_dict(word_weights,orient='index',columns=['rep']).sort_values(by='rep',ascending=False)
  word_weights['cluster']=i
  word_weights_cluster=pd.concat([word_weights_cluster,word_weights ])
  cluster=pd.DataFrame(cluster,columns=['question'])
  cluster['cluster']=i
  clustered_sentences_df=pd.concat([clustered_sentences_df,cluster])

"""# Resumenes para búsqueda en twitter con preguntas asociadas"""

clustered_sentences_key=clustered_sentences_df.merge(df_questions,on='question')
clustered_sentences_key.head()

# guardo la relación de las preguntas al cluster para poder utilizar posteriormente y crear el dataset
clustered_sentences_key.to_csv(path_rdo+'Cluster_preguntas.csv',index=False)

"""### Palabras resumen clusters"""

cluster_4tweets=dict()
for i in range(0,num_clusters):
  # visualizo las 5 palabras más representativas
  print('Cluster'+str(i))
  print('Palabras más representativas')
  print(word_weights_cluster[word_weights_cluster['cluster']==i][0:15])
  print('--')
  print('**Pregunta más representativa**')
  print(summary_clusters[i])
  cluster_4tweets[i]=list(word_weights_cluster[word_weights_cluster['cluster']==i][0:5].index)
  print(cluster_4tweets[i])
  print('--------------------------------------------------')

cluster_4tweets

# guardo las palabras para la busqueda en twitter
pd.DataFrame.from_records(cluster_4tweets).to_csv(path_rdo+'cluster_4tweets.csv')